<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Topographic Sandbox</title>
  <link rel="stylesheet" href="css/styles.css" />
</head>

<body>

  <!-- Sticky Nav -->
  <nav>
    <a onclick="document.getElementById('Project_Objective').scrollIntoView({ behavior: 'smooth' });">
      Project Objective
    </a>
    <a onclick="document.getElementById('video').scrollIntoView({ behavior: 'smooth' });">
      Demo Video
    </a>
    <a onclick="document.getElementById('Introduction').scrollIntoView({ behavior: 'smooth' });">
      Introduction
    </a>
    <a onclick="document.getElementById('Design').scrollIntoView({ behavior: 'smooth' });">
      Design
    </a>
    <a onclick="document.getElementById('Installation Guide').scrollIntoView({ behavior: 'smooth' });">
      Installation Guide
    </a>
    <a onclick="document.getElementById('Testing').scrollIntoView({ behavior: 'smooth' });">
      Testing
    </a>
    <a onclick="document.getElementById('Result').scrollIntoView({ behavior: 'smooth' });">
      Result
    </a>
    <a onclick="document.getElementById('Conclusion').scrollIntoView({ behavior: 'smooth' });">
      Conclusion
    </a>
    <a onclick="document.getElementById('Future Work').scrollIntoView({ behavior: 'smooth' });">
      Future Work
    </a>
    <a onclick="document.getElementById('Budget').scrollIntoView({ behavior: 'smooth' });">
      Budget
    </a>
    <a onclick="document.getElementById('The Team').scrollIntoView({ behavior: 'smooth' });">
      The Team
    </a>
    <a onclick="document.getElementById('References').scrollIntoView({ behavior: 'smooth' });">
      References
    </a>
    <a onclick="document.getElementById('Distribution').scrollIntoView({ behavior: 'smooth' });">
      Distribution
    </a>
  </nav>

  <!-- Hero / Home Section -->
  <header id="home">
    <h1>Topographic Sandbox</h1>
    <div style="height: 20px;"></div>
    <h2>By: George Maidhof (gpm58) and Giorgi Berndt (gb449) </h2>
    <h2>Date: May 10th 2025</h2>
    <div class="scroll-down" onclick="document.getElementById('video').scrollIntoView({ behavior: 'smooth' });">
      ↓ Project Information
    </div>
  </header>

  <section id="Project_Objective">
    <h2>Project Objective</h2>
    <p>
      The goal of this project was to design and build an engaging, interactive sandbox using real-time depth sensing
      and projection.
      Inspired by our childhood experiences, we developed a system that
      encourages user interaction by allowing changes in the sand's shape to be reflected in a dynamic, color-mapped
      projection.
      This project serves as a creative application of concepts learned in class, particularly GPIO control and
      real-time
      data processing with the Raspberry Pi, while also offering a fun demonstration of how physical changes can be
      reflected as inputs and can drive real time changes in a digital
      output in an intuitive and entertaining way.
    </p>
  </section>


  <section id="video" style="padding: 60px 20px; text-align: center;">
    <h2 style="font-size: 2rem; margin-bottom: 20px;">Demo Video</h2>
    <div style="
      display: inline-block;
      background: white;
      border-radius: 16px;
      box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1);
      border: 2px solid #e0e0e0;
      padding: 20px;
      max-width: 600px;
      width: 100%;
    ">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/7OE1gsFn2Js" title="Topographic Sandbox"
        frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen style="border-radius: 12px; width: 100%; height: 315px;"></iframe>
    </div>
  </section>


  <section id="Introduction">
    <h2>Introduction</h2>
    <p>
      We created a sandbox that projects a color map using real-time topographical data using an Xbox 360 Kinect, a
      Raspberry Pi, and a Projector.
      The Kinect captures a continuous stream of depth data in a 480x640 array that are the difference between the Xbox
      Kinect and the sand surface.
      These depth values are processed in real-time and mapped to a texture.
      The resulting color map is projected onto the sand, creating a live display that changes as users interact with
      our sandbox.
      We also integrated GPIO-controlled buttons on the Raspberry Pi to easily start and stop the system.
    </p>
  </section>

  <section id="Design">
    <h2>Design</h2>
    <p>
      This project integrates hardware and software components to create a responsive, real-time augmented reality
      sandbox.
      The system is designed to capture live depth data from a Microsoft Xbox 360 Kinect sensor, process it using a
      Raspberry Pi 4, and project a corresponding color-mapped topographic image onto a physical sandbox.
      The primary goal of the design was to create an interactive system that visually responds to changes in the sand
      surface.
    </p>
    <br />

    <h4>System Architecture Overview</h4>
    <p>
      The core components include the Kinect 1414 depth sensor, a Raspberry Pi 4, a vertically-mounted projector, and
      physical GPIO buttons on a TFT for control.
      The Kinect captures a live 480x640 stream of depth data which is sent to the Raspberry Pi via USB 3.0.
      The Raspberry Pi processes the data and generates a texture where the color corresponds to terrain elevation,
      which
      is then projected onto the sandbox.
      GPIO buttons connected to the Pi allow users to start and stop the visualization system.
    </p>
    <br />

    <h4>Hardware Design</h4>
    <p>
      Depth Sensor:
      The Xbox 360 Kinect 1414 serves as our depth sensor.
      The Kinect provides a continuous stream of depth data to the Raspberry Pi 4 over USB 3.0 representing the
      difference between the Kinect and the sandbox.
    </p>
    <br>
    <p>
      Projection Setup:
      A standard overhead projector, provided by Professor Skovira, is mounted above the sandbox and aligned vertically
      with the sandbox in a manner to minimize distortion. </p>
    <br>
    <p>
      GPIO Controls:
      GPIO buttons exposed by the TFT are accessible by the Raspberry Pi.
      The GPIO pins are used to start and stop the system.
      This allows the user to initiate or terminate the program without using a keyboard and mouse.
    </p>
    Refer to the flow diagram below for a visual representation of the system hardware architecture:
    <div style="text-align: center; margin-top: 20px;">
      <img src="photos/flow_diagram.png" alt="System Architecture Diagram"
        style="width: 80%; max-width: 400px; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1);">
      <br />

      <h4>Software Architecture</h4>
      <p>
        The system software is implemented using a combination of C and Python, leveraging the strengths of each
        language:
      </p>
      <br>
      <p>
        C Code: Handles all performance-critical tasks such as depth data acquisition and color mapping. This choice was
        made to improve real-time responsiveness given the Raspberry Pi’s limited processing power. We used the
        libfreenect library to utilize all four cores on the Pi and get the depth data from the Kinect efficiently. This
        allowed for the real time performance that is seen in our demo.
      </p>
      <br>
      <p>
        Python Script: Manages GPIO input and user control of the system. The Python code waits for GPIO button presses
        and on command begins the C code subprocess. It can also be used to terminate the program.
      </p>
      <br />

      <h4>Data Processing Pipeline</h4>
      <p>
        Once the system is initialized via the start button:
      </p>
      <p>
        1) The Kinect begins streaming depth data.
      </p>
      <br>
      <p>
        2) The Kinect device context <code>freenect_context</code> is initialized and it opens
        <code>freenect_device</code>.
      </p>

      <details
        style="margin-top: 20px; background: #fff; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.08); overflow: hidden; transition: all 0.3s ease;">
        <summary
          style="cursor: pointer; font-weight: 600; padding: 12px 16px; background: linear-gradient(135deg, #ff9999, #ff4d4d); color: white; font-size: 1rem;">
          View Code Block
        </summary>
        <pre
          style="margin: 0; padding: 16px; background-color: #f9f9f9; font-size: 0.95rem; line-height: 1.5; overflow-x: auto;">
    <code style="color: #333;">
    if (freenect_init(&f_ctx, NULL) &lt; 0) {
        printf("Freenect_Init Not Working\n");
        return -1;
    }
    
    if (freenect_open_device(f_ctx, &f_dev, 0) &lt; 0) {
        printf("Freenect Open Device Not Working\n");
        return -1;
    }
    
    if (init_sdl() &lt; 0) {
        return -1;
    }
    </code>
      </pre>
      </details>
      <br>
      <p>
        3) The SDL2 window, renderer, and texture are initialized for displaying
        the depth data. The function init_sdl will create a window
        called "Kinect Depth Viewer" with dimensions 1480x1080. Then it
        will create a renderer and a texture for rendering depth data
        using RGB24 pixel format of dimensions 480x640.
      </p>
      <details
        style="margin-top: 20px; background: #fff; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.08); overflow: hidden; transition: all 0.3s ease;">
        <summary
          style="cursor: pointer; font-weight: 600; padding: 12px 16px; background: linear-gradient(135deg, #ff9999, #ff4d4d); color: white; font-size: 1rem;">
          View Code Block
        </summary>
        <pre
          style="margin: 0; padding: 16px; background-color: #f9f9f9; font-size: 0.95rem; line-height: 1.5; overflow-x: auto;">
  <code style="color: #333;">
    int init_sdl() {
      if (SDL_Init(SDL_INIT_VIDEO) &lt; 0) {
          fprintf(stderr, "SDL Init Not Working: %s\n", SDL_GetError());
          return -1;
      }
  
      window = SDL_CreateWindow("Kinect Depth Viewer", 500, SDL_WINDOWPOS_UNDEFINED, 1480, 1080, 0);
      renderer = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED);
      texture = SDL_CreateTexture(renderer, SDL_PIXELFORMAT_RGB24, SDL_TEXTUREACCESS_STREAMING, WIDTH, HEIGHT);
  
      if (!window || !renderer || !texture) {
          fprintf(stderr, "SDL Not Working\n");
          return -1;
      }
  
      return 0;
  }
  </code>
    </pre>
      </details>
      <br>
      <p>
        4) Next we used the depth callback function using freenect_set_depth_callback
        to call depth_cb. This receives the depth data from the Kinect device and
        processes each pixel.
      </p>
      <br>
      <p>
        5) Each depth value is checked to be within the range 1100 to 1700 and
        then is normalized. This normalized value is mapped to RGB colors
        and assigns those RGB values to the corresponding pixel in the texture.
        If the value is out of range then the color is assigned to be grey. Based on the orientation of the projector
        and
        kinect, we had to flip the image 180 degrees as well.
      </p>
      <details
        style="margin-top: 20px; background: #fff; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.08); overflow: hidden; transition: all 0.3s ease;">
        <summary
          style="cursor: pointer; font-weight: 600; padding: 12px 16px; background: linear-gradient(135deg, #ff9999, #ff4d4d); color: white; font-size: 1rem;">
          View Code Block
        </summary>
        <pre
          style="margin: 0; padding: 16px; background-color: #f9f9f9; font-size: 0.95rem; line-height: 1.5; overflow-x: auto;">
  <code style="color: #333;">
    void depth_cb(freenect_device *dev, void *depth, uint32_t timestamp){
      uint16_t *d = (uint16_t*)depth;
      unsigned char rgb[WIDTH * HEIGHT * 3];
  
      for (int y = 0; y &lt; HEIGHT; ++y) {
          for (int x = 0; x &lt; WIDTH; ++x) {
              uint16_t v = d[y * WIDTH + x];
              int index = (y * WIDTH + x) * 3;
  
              if (v >= 1100 && v &lt;= 1700) {
                  float norm = 1.0f - (float)(v - 1300) / 100.0f; 
                  float r = fminf(fmaxf(1.5f - fabsf(4.0f * norm - 3.0f), 0.0f), 1.0f);
                  float g = fminf(fmaxf(1.5f - fabsf(4.0f * norm - 2.0f), 0.0f), 1.0f);
                  float b = fminf(fmaxf(1.5f - fabsf(4.0f * norm - 1.0f), 0.0f), 1.0f);
  
                  rgb[index + 0] = (uint8_t)(r * 255);
                  rgb[index + 1] = (uint8_t)(g * 255);
                  rgb[index + 2] = (uint8_t)(b * 255);
              } else {
                  rgb[index + 0] = 64;
                  rgb[index + 1] = 0;
                  rgb[index + 2] = 0;
              }
          }
      }
  
      SDL_UpdateTexture(texture, NULL, rgb, WIDTH * 3);
      SDL_RenderClear(renderer);
  
      SDL_Rect src_rect = {160, 120, 320, 240};
  
      SDL_Rect dst_rect = {0, 0, 1480, 1080};
  
      SDL_RenderCopyEx(renderer, texture, &src_rect, &dst_rect, 0, NULL, SDL_FLIP_HORIZONTAL | SDL_FLIP_VERTICAL);
      SDL_RenderPresent(renderer);
  }
  </code>
    </pre>
      </details>
      <br>
      <p>
        6) The final color-mapped image is sent to the projector, which overlays the visual representation onto the
        sand.
      </p>
      <br>
      <p>
        This loop repeats continuously, ensuring real-time updates as users manipulate the sand.
      </p>
      <br />

      <h4>Projection Mapping and Calibration</h4>
      <p>
        Projection alignment was a key part of the design process.
        Our sandbox has a square shape, while the camera feed and image output are rectangular.
        When booting from the Pi, we had to manually adjust the height and angle of the projector to ensure that the
        projected image correctly covered
        the sandbox area. This could simply be fixed by changing the mount to move a little forward and to the right.
        However, when running it from our computers, we could provide a digital offset to ensure accurate rendering.
        The depth range was also calibrated to match the typical height variations achievable within the sandbox.
      </p>
      <br />

      <h4>Real-Time Optimization</h4>
      <p>
        Given the limited computational resources of the Raspberry Pi, we prioritized efficiency:
      </p>
      <br>
      <p>
        All computationally intensive tasks, especially per-frame depth processing and color mapping, were implemented
        in
        C.
      </p>
      <br>
      <p>
        Python was used only where performance was not critical (GPIO control).
      </p>
      <br />

      <h4>Challenges and Solutions</h4>
      <p>
        Performance Bottlenecks: To enable real time performance, we both utilized the libfreenect library to use all 4
        of
        the
        Raspberry Pi's cores. In addition, we found that the Lab 4 kernel removed certain functionalities
        that were necessary for high performance. As a result, the Lab 3 kernel had files that
        enhanced our performance that were previously removed from the Lab 4 kernel.
      </p>
      <br>
      <p>
        Projection Alignment:
        On the computer, we could just set an offset to accurately map onto the sandbox. However, when booting from
        the Raspberry Pi, we had to make physical adjustments to the
        projector in order to try to align our projection to the sand without having an offset.
      </p>
      <br>
      <p>
        GPIO Responsiveness:
        Handling button presses with Python's GPIO library worked reliably after implementing and
        ensuring the subprocess management was clean. We were originally going to use touchscreen button functionality
        but
        it did not originally work with the Lab 4 kernel which is why we had swapped to the Lab 3 kernel. When doing
        testing for the TFT touchscreen button functionality, we were unable to spawn two windows (one on the TFT and
        one
        through the Projector) at the same time which
        is why we took the GPIO button approach.
      </p>
      <br>
      <p>
        Boot From Start:
        We found that we had issues when using Crontab in order to automate our system as everything we had tried, even
        using direct path names resulted in nothing working. We eventually swapped from using Crontab to using bashrc
        which
        worked perfectly. With this though, we had lost some fuctionality where the window would spawn off center but
        this
        could be fixed mannually.
      </p>
  </section>

  <section id="Installation Guide"
    style="background-color: #f9f9f9; padding: 40px 20px; border-radius: 12px; box-shadow: 0 4px 8px rgba(0,0,0,0.05); margin: 40px auto; max-width: 900px;">
    <h2 style="text-align: center; color: #2c3e50; margin-bottom: 25px;">Installation Guide</h2>

    <br>
    <ol style="text-align: left; padding-left: 20px; font-size: 16px; color: #444; line-height: 1.7;">
      <li>
        <span style="font-weight: bold; text-decoration: underline;">Start with the Lab 3 Kernel</span>: When starting
        our project, we suggest you to start with the Lab 3 kernel it performed the best for us speed wise and also
        worked with the PiTFT.
      </li>
      <br>
      <li>
        <span style="font-weight: bold; text-decoration: underline;">Install Dependencies</span>: Below are the
        necessary dependencies to get the Xbox 360 Kinect 1414 to work on the Raspberry Pi 4.
        <pre
          style="background-color: #f4f4f4; padding: 10px; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0,0,0,0.1); overflow-x: auto; margin-top: 8px;">
        <code>
          sudo apt update
          sudo apt upgrade

          sudo apt install libfreenect-dev libsdl2-dev libusb-1.0-0-dev
          sudo apt install python2 python2-dev
          sudo python2 -m pip install cython
          curl https://bootstrap.pypa.io/pip/2.7/get-pip.py --output get-pip.py
          sudo python2 get-pip.py
          sudo python2 -m pip install cython
          sudo apt install -y cmake build-essential pkg-config
        </code>
        </pre>
      </li>
      <br>
      <li>
        <span style="font-weight: bold; text-decoration: underline;">Downloading and Build OpenKinect's
          Libfreenect</span>:
        In your home directory (home/pi/) and install the OpenKinect's Libfreenect and extract it and its dependencies
        by doing the following:
        <pre
          style="background-color: #f4f4f4; padding: 10px; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0,0,0,0.1); overflow-x: auto; margin-top: 8px;">
        <code>
          git clone https://github.com/OpenKinect/libfreenect.git
          cd libfreenect
          mkdir build && cd build
          cmake .. -DBUILD_EXAMPLES=ON -DBUILD_CPP=ON
          make -j4
          sudo make install
        </code>
        </pre>
      </li>
      <br>
      <li>
        <span style="font-weight: bold; text-decoration: underline;">Using Cython for Bindings and Wrappers</span>:
        Upon installed and building libfreenect, you will need to install Cython for the bindings and wrappers in order
        to utilize the libfreenect in your code. Perform the following and then if done correclty, your enviorment
        should work as intended:
        <pre
          style="background-color: #f4f4f4; padding: 10px; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0,0,0,0.1); overflow-x: auto; margin-top: 8px;">
        <code>
          cd ../wrappers/python
          cython freenect.pyx
          sudo python3 setup.py install
        </code>
        </pre>
        <br>
      </li>
      <br>
    </ol>
  </section>


  <section id="Testing">
    <h2>Testing</h2>
    <p>
      For our testing, we broke down the project into several components.
      First, we had to get the depth sensors working. For that we tested out
      several libraries where we ended up doing the project through libfreenect, an open source Linux library for the
      Xbox 360 Kinect.
      We then tried doing basic rendering of the scene by
      setting a threshold value and setting all values below that value to red
      and all values above it to blue after successfully being able to read depth data. After confirming and getting
      that working
      we then moved on to calibrating the projector from the computer.
      This was done to make sure we could properly align the colors with
      the map and make sure that the depth sensor and projector could be aligned together.
      Next, we then debugged implementing real time functionality.
      The libfreenect library allowed us to use all four cores on the Pi
      and interestingly, using the kernel from Lab 3 as opposed to Lab 4
      also sped up performance significantly. Lastly, we then just implemented
      GPIO button as we had regained functionality from the PiTFT when swapping to the Lab 3 kernel. We used this
      functionality so that we could use
      the Pi to initialize and
      terminate the project from boot.
    </p>
  </section>

  <section id="Result">
    <h2>Result</h2>
    <h4>System Performance and Accuracy</h4>
    <p>
      We were able to implement real-time depth mapping given that the Kinect
      sensor captured the depth data and the libfreenect library enabled
      the system to generate accurate topographic maps in real-time.
      In addition, our use of C for all time critical components of
      the program enabled its high performance. Lastly,
      implementing an offset on the computer allowed for accurate
      projection alignment. However, when booting from the Pi we
      had to make manual adjustments to try to improve its accuracy. As for the development of our code over time, some
      photos attached below highlight our adjustment and progression over time to meet the standards for this project we
      desired.
    </p>
    <div style="display: flex; justify-content: center; gap: 20px; margin-top: 30px; flex-wrap: nowrap;">
      <div style="text-align: center; flex: 1;">
        <img src="photos/iteration1.jpg" alt="Iteration 1"
          style="width: 260px; height: 300px; object-fit: cover; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
        <p style="margin-top: 8px; font-size: 0.95rem;">Iteration 1: Data abstracted and used only red and blue (2 solid
          colors)</p>
      </div>
      <div style="text-align: center; flex: 1;">
        <img src="photos/iteration2.jpg" alt="Iteration 2"
          style="width: 260px; height: 300px; object-fit: cover; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
        <p style="margin-top: 8px; font-size: 0.95rem;">Iteration 2: Added multi-dimensional colors</p>
      </div>
      <div style="text-align: center; flex: 1;">
        <img src="photos/iteration3.jpg" alt="Iteration 3"
          style="width: 260px; height: 300px; object-fit: cover; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
        <p style="margin-top: 8px; font-size: 0.95rem;">Iteration 3: More fine-tuned (steeper gradient) and inverted
          colors (blue deeper, red higher)</p>
      </div>
    </div>



    <br>
    <h4>User Interaction and Engagement</h4>
    <p>
      Users were able to engage with our project by actively shaping the
      sand in the sandbox to create various topographic features.
      They were able to observe immediate changes in the projected map
      that reflected the changes that they had created in the sand. Through visually tesitng, we were able to determine
      its effectivness and accuracy.
    </p>
    <br>
    <h4>Challenges Encountered</h4>
    <p>
      We faced several difficult challenges. Primarily amongst it was
      determining how to interface with the XBox 360 Kinect. In addition,
      how to process the data at real-time speeds was another challenge.
      Lastly, attempting to calibrate the projector as well as building
      the physical sandbox were additional challenges we had to overcome.
    </p>
    <br>
    <h4>Summary</h4>
    <p>
      Overall, you can see the results of our project shared below
      with an image of us together next to the sandbox as well as a
      video of us testing it out.
    </p>

    <div
      style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap; align-items: center; margin-top: 20px;">

      <img src="photos/IMG_4871.jpeg" alt="Team with Sandbox"
        style="width: 400px; height: 700px; object-fit: cover; object-position: left; border-radius: 10px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);">


      <video controls
        style="width: 400px; height: 700px; object-fit: cover; border-radius: 10px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);">
        <source src="photos/IMG_4872.mp4" type="video/mp4">
      </video>
    </div>
  </section>

  <section id="Conclusion">
    <h2>Conclusion</h2>
    <p>
      In completing this project, we were able to successfully create a topographic sandbox on a linux based operating
      system through using a depth camera from an XBox 360 Kinect and a Raspberry Pi 4. Our final project helped us
      learn how to tailor a solution for an uncertain desing process, learn to adjust to the limitations of the
      hardware, all within a timely manner. We were able to create a close to real-time topographic map of the sandbox
      with minimal delay and utilization of multi-core to solve a hardware problem even though it was handled through a
      open source library. Also, as an Electrical and Computer Engineer and a Computer Scientist, we develoepd
      mechanical skills on assembly of a sturdy frame and mount for the Kinect and projector.
      <br>
      <br>
      Some mistakes that we made along the way included some mistakes on the mounting of the projector resulting in some
      off center behavior. Also, when working in the Lab 4 kernel, we ran into difficulties with getting certain
      librarbies to work that we needed including Pygame and envtest. When prepping, we should have verified that these
      we including before realizing later on, resulting in a swap to a older kernel close to our due date. Lastly, by
      playing around with our operating system for boot last minute, we had made some mistakes to the system that
      resulted in our "startx" command not working as intended, though the project still worked. With that being said,
      our biggest takeaway is to always test our system step by step along with keeping a backup of the system in case
      things go wrong!
    </p>
  </section>

  <section id="Future Work"
    style="background-color: #f9f9f9; padding: 40px 20px; border-radius: 12px; box-shadow: 0 4px 8px rgba(0,0,0,0.05); margin: 40px auto; max-width: 900px;">
    <h2 style="text-align: center; color: #2c3e50; margin-bottom: 25px;">Future Work</h2>
    <p style="font-size: 17px; color: #333; line-height: 1.6;">
      Some small flaws we ran into that we would either like to address or improve are as follows:
    </p>
    <br>
    <ol style="text-align: left; padding-left: 20px; font-size: 16px; color: #444; line-height: 1.7;">
      <li>
        <span style="font-weight: bold; text-decoration: underline;">Centering the projector to the sandbox</span>:
        Starting our program from boot via <code>.bashrc</code> had side effects that caused the window to spawn
        off-center. This slightly shifted the projection upward and to the right. We could fix this simply by physically
        adjusting the projector mount.
      </li>
      <li>
        <span style="font-weight: bold; text-decoration: underline;">Adding touchscreen buttons</span>:
        Initially, we couldn't use the PiTFT with our Lab 4 kernel, which lacked Pygame support. After switching to the
        Lab 3 kernel, the TFT worked, but due to time constraints, we used GPIO buttons instead of touchscreen GUI
        buttons. Adding a touchscreen interface would be something else we would want to implement.
      </li>
      <li>
        <span style="font-weight: bold; text-decoration: underline;">Deeper color mapping</span>:
        We implemented a color range from red to blue, but additional enhancements could include gradient rings for more
        distinct terrain lines and a richer color palette.
      </li>
    </ol>
  </section>

  <section id="Budget">
    <h2>Budget</h2>
    <p>The materials for our project revolved around making use of any unused materials lying around within the ECE
      Department and the loading doc. In the short section below, we will describe the materials used, their purpose and
      where they were gotten.
      <br>
      <br>
      In ECE 5725, the main device utilized is a Raspberry Pi V4 along with a 16 GB MicroSD card, a 15W USB-C for power,
      and a resistive PiTFT that mounts on the Raspberry Pi V4 Pins. The rest of the items listed are used for our
      specific Topographic Map Sandbox project.
      <br>
      <br>
      We developed a frame and mount from wood, tape, and screws along with a cardboard bottom to hold the sand. This in
      total would cost you about $12, however we made us of materials left over from the loading dock, along with some
      extra materials Professor Skovira had around. We had purchased 100 lbs of play sand from Home Depot but only used
      40lbs for our project, resulting in about $6.39 worth of sand used. The XBox 360 was purchased off of Ebay with
      the needed wires (power and USB-3.0) for $10.99. For the last of the materials that made up this project, we used
      a small table which we had borrowed from Professor Skovira along with a projector Cornell IT was not using.
      <br>
      <br>
      With this, our total price was about $115.38 dollars. If we were to exclude all of the provided materials
      (Raspberry Pi V4, 16 GB MicroSD card, 15W USB-C power supply, resistive PiTFT, table and projector), the total
      cost of the project would be, about $29.38.
    </p>
    <div
      style="background-color: white; padding: 20px; margin-top: 20px; width: 80%; max-width: 800px; margin: 0 auto; border-radius: 10px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); text-align: left;">
      <h3 style="color: #007BFF; margin-bottom: 20px;">Materials</h3>
      <table style="width: 100%; border-collapse: collapse; margin-bottom: 20px;">
        <tr>
          <th style="border: 1px solid #ddd; padding: 10px; text-align: left; background-color: #007BFF; color: white;">
            Component</th>
          <th style="border: 1px solid #ddd; padding: 10px; text-align: left; background-color: #007BFF; color: white;">
            Estimate (USD)</th>
        </tr>
        <tr>
          <td style="border: 1px solid #ddd; padding: 10px;">Raspberry Pi V4</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$35</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ddd; padding: 10px;">16 GB MicroSD Card</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$8</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ddd; padding: 10px;">15W USB-C Power Supply Cable</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$8</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ddd; padding: 10px;">Resistive Touch PiTFT (320x240) 2.8"</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$35</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ddd; padding: 10px;">Frame (Wood + Tape + Cardboard) + Screws</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$12</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ddd; padding: 10px;">Sand (40 lbs)</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$6.39</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ddd; padding: 10px;">Xbox 360 Kinect Model 1414</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$10.99</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ddd; padding: 10px;">Table (Borrowed)</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$0</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ddd; padding: 10px;">Projector (From IT)</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$0</td>
        </tr>
        <tr style="font-weight: bold; background-color: #e0e0e0;">
          <td style="border: 1px solid #ddd; padding: 10px;">Total Cost Estimate</td>
          <td style="border: 1px solid #ddd; padding: 10px;">$115.38</td>
        </tr>
      </table>
    </div>
  </section>

  <section id="The Team">
    <h2>The Team</h2>
    <div style="display: flex; justify-content: center; gap: 40px; flex-wrap: wrap; margin-top: 30px;">
      <div
        style="background: white; padding: 20px; border-radius: 16px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); width: 250px; text-align: center;">
        <img src="photos/the_team_photos/george.jpg" alt="Member 1"
          style="width: 100%; border-radius: 12px; object-fit: cover; height: 300px; margin-bottom: 15px;">
        <h3 style="margin-bottom: 5px;">George Maidhof (gpm58)</h3>
        <p style="color: #777;">Electrical and Computer Engineering</p>
      </div>

      <div
        style="background: white; padding: 20px; border-radius: 16px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); width: 250px; text-align: center;">
        <img src="photos/the_team_photos/giorgi.jpeg" alt="Member 2"
          style="width: 100%; border-radius: 12px; object-fit: cover; height: 300px; margin-bottom: 15px;">
        <h3 style="margin-bottom: 5px;">Giorgi Berndt (gb449)</h3>
        <p style="color: #777;">Computer Science</p>
      </div>
    </div>
  </section>

  <section id="References">
    <h2>References</h2>
    <p>
      [1] C. Beals, “How to Build an Augmented Reality Sandbox,” bealsscience. Accessed: May 10, 2025. [Online].
      Available: https://www.bealsscience.com/post/2017/06/07/augmented-reality-sandbox-will-blow-your-mind
      <br />
      [2] “OpenKinect.” Accessed: May 10, 2025. [Online]. Available: https://github.com/OpenKinect
      <br />
      [3] “SDL library in C/C++ with examples,” GeeksforGeeks. Accessed: May 10, 2025. [Online]. Available:
      https://www.geeksforgeeks.org/sdl-library-in-c-c-with-examples/
      <br />
      [4] “SDL2/SDL_Color.” Accessed: May 11, 2025. [Online]. Available: https://wiki.libsdl.org/SDL2/SDL_Color
      <br />
      [5] OpenAI, ChatGPT, San Francisco, CA, USA. [Online]. Available: https://chat.openai.com/ (Specifically for help
      with HTML and website formating, not text generation)
      <br />
      [6] J. Skovira, “ECE 5725 Lab 1,” 2025. Lab 1_Spring2025_v2.pdf. Accessed: May 10, 2025. [Online].
      <br />
      [7] J. Skovira, “ECE 5725 Lab 3,” 2025. Lab 3_Spring2025_v2.pdf. Accessed: May 10, 2025. [Online].
    </p>
  </section>

  <section id="Distribution">
    <h2>Distribution</h2>
    <p>
      Given that George Maidhof is an ECE Major and Giorgi Berndt is a CS Major,
      George spearheaded the work on the hardware side while Giorgi focused on
      the software side. Given Giorgi was traveling some of the last week prior to the
      competition, George ended up spending more time in the lab
      so Giorgi made up for it by authoring most of the work on the website.
    </p>
  </section>

  <section id="The Code Listing">
    <h2>The Code Listing</h2>
    <p>
    <details
      style="margin-top: 20px; background: #fff; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.08); overflow: hidden; transition: all 0.3s ease;">
      <summary
        style="cursor: pointer; font-weight: 600; padding: 12px 16px; background: linear-gradient(135deg, #ff9999, #ff4d4d); color: white; font-size: 1rem;">
        C Code:
      </summary>
      <pre
        style="margin: 0; padding: 16px; background-color: #f9f9f9; font-size: 0.95rem; line-height: 1.5; overflow-x: auto;">
  <code style="color: #333;">
      #include &lt;stdio.h&gt;
      #include &lt;stdlib.h&gt;
      #include &lt;libfreenect.h&gt;
      #include &lt;unistd.h&gt;
      #include &lt;libusb-1.0/libusb.h&gt;
      #include &lt;SDL2/SDL.h&gt;
      #include &lt;math.h&gt;
      
      #define WIDTH  640
      #define HEIGHT 480
      
      freenect_context *f_ctx;
      freenect_device *f_dev;
      
      SDL_Window *window = NULL;
      SDL_Renderer *renderer = NULL;
      SDL_Texture *texture = NULL;
      
      int init_sdl() {
          if (SDL_Init(SDL_INIT_VIDEO) < 0) {
              fprintf(stderr, "SDL Init Not Working: %s\n", SDL_GetError());
              return -1;
          }
          
          window = SDL_CreateWindow("Kinect Depth Viewer", 500, SDL_WINDOWPOS_UNDEFINED, 1480, 1080, 0);
          renderer = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED);
          texture = SDL_CreateTexture(renderer, SDL_PIXELFORMAT_RGB24, SDL_TEXTUREACCESS_STREAMING, WIDTH, HEIGHT);
        
          if (!window || !renderer || !texture) {
              fprintf(stderr, "SDL Not Working\n");
              return -1;
          }
        
          return 0;
      }
      
      void cleanup_sdl() {
          SDL_DestroyTexture(texture);
          SDL_DestroyRenderer(renderer);
          SDL_DestroyWindow(window);
          SDL_Quit();
      }
      
      void depth_cb(freenect_device *dev, void *depth, uint32_t timestamp){
          uint16_t *d = (uint16_t*)depth;
          unsigned char rgb[WIDTH * HEIGHT * 3];
      
          for (int y = 0; y &lt; HEIGHT; ++y) {
              for (int x = 0; x &lt; WIDTH; ++x) {
                  uint16_t v = d[y * WIDTH + x];
                  int index = (y * WIDTH + x) * 3;
      
                  if (v >= 1100 && v &lt;= 1700) {
                      float norm = 1.0f - (float)(v - 1300) / 100.0f; 
                      float r = fminf(fmaxf(1.5f - fabsf(4.0f * norm - 3.0f), 0.0f), 1.0f);
                      float g = fminf(fmaxf(1.5f - fabsf(4.0f * norm - 2.0f), 0.0f), 1.0f);
                      float b = fminf(fmaxf(1.5f - fabsf(4.0f * norm - 1.0f), 0.0f), 1.0f);
      
                      rgb[index + 0] = (uint8_t)(r * 255);
                      rgb[index + 1] = (uint8_t)(g * 255);
                      rgb[index + 2] = (uint8_t)(b * 255);
                  } else {
                      rgb[index + 0] = 64;
                      rgb[index + 1] = 0;
                      rgb[index + 2] = 0;
                  }
              }
          }
      
          SDL_UpdateTexture(texture, NULL, rgb, WIDTH * 3);
          SDL_RenderClear(renderer);
      
          SDL_Rect src_rect = {160, 120, 320, 240};
      
          SDL_Rect dst_rect = {0, 0, 1480, 1080};
      
          SDL_RenderCopyEx(renderer, texture, &src_rect, &dst_rect, 0, NULL, SDL_FLIP_HORIZONTAL | SDL_FLIP_VERTICAL);
          SDL_RenderPresent(renderer);
      }
      
      int main(){
          if (freenect_init(&f_ctx, NULL) &lt; 0) {
              printf("Freenect_Init Not Working\n");
              return -1;
          }
          
          if (freenect_open_device(f_ctx, &f_dev, 0) &lt; 0) {
              printf("Freenect Open Device Not Working\n");
              return -1;
          }
          
          if (init_sdl() &lt; 0) {
              return -1;
          }
      
          freenect_set_depth_mode(f_dev, freenect_find_depth_mode(FREENECT_RESOLUTION_MEDIUM, FREENECT_DEPTH_MM));
          freenect_set_depth_callback(f_dev, depth_cb);
          freenect_start_depth(f_dev);
      
          while (1) {
              SDL_Event e;
              while (SDL_PollEvent(&e)) {
                  if (e.type == SDL_QUIT) goto quit;
              }
      
              if (freenect_process_events(f_ctx) &lt; 0) {
                  fprintf(stderr, "Error processing events\n");
                  break;
              }
          }
      
      quit:
          freenect_stop_depth(f_dev);
          freenect_close_device(f_dev);
          freenect_shutdown(f_ctx);
          cleanup_sdl();
          return 0;
      }      
  </code>
    </pre>
    </details>
    <br>
    <details
      style="margin-top: 20px; background: #fff; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.08); overflow: hidden; transition: all 0.3s ease;">
      <summary
        style="cursor: pointer; font-weight: 600; padding: 12px 16px; background: linear-gradient(135deg, #ff9999, #ff4d4d); color: white; font-size: 1rem;">
        Python Code:
      </summary>
      <pre
        style="margin: 0; padding: 16px; background-color: #f9f9f9; font-size: 0.95rem; line-height: 1.5; overflow-x: auto;">
  <code style="color: #333;">
      import RPi.GPIO as GPIO
      import time
      import subprocess
      import os
      import signal
      import sys

      GPIO.setmode(GPIO.BCM)

      START_BUTTON = 17
      QUIT_BUTTON = 22
      QUIT_PYTHON = 27

      GPIO.setup(START_BUTTON, GPIO.IN, pull_up_down=GPIO.PUD_UP)
      GPIO.setup(QUIT_BUTTON, GPIO.IN, pull_up_down=GPIO.PUD_UP)
      GPIO.setup(QUIT_PYTHON, GPIO.IN, pull_up_down=GPIO.PUD_UP)


      kinect_process = None

      try:
          while True:
              if GPIO.input(START_BUTTON) == GPIO.LOW:
                  if kinect_process is None or kinect_process.poll() is not None:
                      print("Starting Kinect")
                      kinect_process = subprocess.Popen(["/home/pi/Final_Project/kinect_depth"])
                      time.sleep(0.5)  

              if GPIO.input(QUIT_BUTTON) == GPIO.LOW:
                  if kinect_process is not None and kinect_process.poll() is None:
                      print("Stopping Kinect")
                      kinect_process.send_signal(signal.SIGINT)
                      kinect_process.wait()
                      kinect_process = None
                      time.sleep(0.5)
                      
              if GPIO.input(QUIT_PYTHON) == GPIO.LOW:
                  if True:
                      print("Quitting Python Script")
                      GPIO.cleanup()
                      sys.exit()
                      time.sleep(0.5)
              

              time.sleep(0.1)

      except KeyboardInterrupt:
        print("\nExiting program")

      finally:
          if kinect_process is not None and kinect_process.poll() is None:
              kinect_process.terminate()
          GPIO.cleanup()
  </code>
    </pre>
    </details>
    <br>
    <details
      style="margin-top: 20px; background: #fff; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.08); overflow: hidden; transition: all 0.3s ease;">
      <summary
        style="cursor: pointer; font-weight: 600; padding: 12px 16px; background: linear-gradient(135deg, #ff9999, #ff4d4d); color: white; font-size: 1rem;">
        Bash Script:
      </summary>
      <pre
        style="margin: 0; padding: 16px; background-color: #f9f9f9; font-size: 0.95rem; line-height: 1.5; overflow-x: auto;">
  <code style="color: #333;">
    #!/bin/bash

    sudo python /home/pi/Final_Project/gpio_control.py
  </code>
    </pre>
    </details>
    </p>
  </section>

  <!-- Scroll Effects Script -->
  <script>
    const handlesLogos = document.querySelector('.handles-logos');

    function revealOnScroll() {
      const triggerBottom = window.innerHeight * 0.85;
      const boxTop = handlesLogos.getBoundingClientRect().top;
      if (boxTop < triggerBottom) {
        handlesLogos.classList.add('visible');
      }
    }

    function highlightNav() {
      const sections = document.querySelectorAll("section");
      const navLinks = document.querySelectorAll("nav a");

      let current = "home";
      sections.forEach((section) => {
        const top = window.scrollY + 100;
        const offset = section.offsetTop;
        if (top >= offset) {
          current = section.getAttribute("id");
        }
      });

      navLinks.forEach((link) => {
        link.classList.remove("active");
        if (link.getAttribute("href") === `#${current}`) {
          link.classList.add("active");
        }
      });
    }

    window.addEventListener("scroll", () => {
      revealOnScroll();
      highlightNav();
    });

    window.addEventListener("load", () => {
      revealOnScroll();
      highlightNav();
    });
  </script>

</body>

</html>